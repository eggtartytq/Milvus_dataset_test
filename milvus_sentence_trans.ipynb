{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc667f4-9400-4a40-ba2e-44d75ca2b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, utility, MilvusException, FieldSchema, CollectionSchema, DataType, Collection, loading_progress\n",
    "connections.connect(host=\"localhost\", port=\"19530\", db_name='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a5f1e-dbd1-47a6-aab3-4e9015c70ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    FieldSchema(name=\"Data_Id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"Dataset_File\", dtype=DataType.VARCHAR,max_length=255),\n",
    "    FieldSchema(name=\"Collected\", dtype=DataType.VARCHAR,max_length=100),\n",
    "    FieldSchema(name=\"Variable\", dtype=DataType.VARCHAR,max_length=100),\n",
    "    FieldSchema(name=\"Label\", dtype=DataType.VARCHAR,max_length=255),\n",
    "    FieldSchema(name=\"Description\", dtype=DataType.VARCHAR,max_length=1024),\n",
    "    FieldSchema(name=\"Info_embedding\", dtype=DataType.FLOAT_VECTOR, dim=768),\n",
    "]\n",
    "\n",
    "collection_schema = CollectionSchema(fields, description=\"Example_collection\")\n",
    "\n",
    "doc_data = Collection(\"Milvus_Test_Sentrans\", collection_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8481eca-b088-4050-9509-972e735cba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index on vector\n",
    "def Create_Index(collection_Name: str, vector_name: str, index_par: dict):\n",
    "    try:\n",
    "        collection = Collection(collection_Name)\n",
    "        collection.create_index(field_name = vector_name, index_params = index_par)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        print(\"Index created\")\n",
    "\n",
    "index_params = {\n",
    "    \"index_type\" : \"IVF_FLAT\",\n",
    "    \"metric_type\" : \"L2\",\n",
    "    \"params\" : {\"nlist\":128}  ,\n",
    "}\n",
    "\n",
    "Create_Index(\"Milvus_Test_Sentrans\", \"Info_embedding\", index_params) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38a57-a2e0-4b68-b2e7-558a9d1dcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Collection_Name = 'Milvus_Test_Sentrans'\n",
    "collection = Collection(Collection_Name)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7302384-699a-4ea1-91f7-3ae12f960220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert function \n",
    "#NOT TEST YET\n",
    "def Insert_vector(id_list: list, data_file_list: list, collected_list: list, variable_list: list, label_list: list, Description_list: list, embeddings_Description, Collection):\n",
    "    try:\n",
    "        collection = Collection\n",
    " \n",
    "        for id_item, data_file_item, collected_item, variable_item, label_item, embeddings_Description in zip(id_list, data_file_list, collected_list, variable_list, label_list, embeddings_Description):\n",
    "            entities = [[id_item], [data_file_item], [collected_item], [variable_item], [label_item], [embeddings_Description]]\n",
    "            collection.insert(entities)\n",
    "            print(f\"编号{id_item}插入完毕\")\n",
    "        collection.flush()\n",
    " \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6872c7d-f547-4f01-a5b0-ef4fd66f6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Insert_vector2(id_list: list, data_file_list: list, collected_list: list, variable_list: list, label_list: list, Description_list: list, embeddings_Description, Collection):\n",
    "    try:\n",
    "        collection = Collection\n",
    " \n",
    "        for id in id_list:\n",
    "            index = id - 1\n",
    "            entities = [[id_list[index]], [data_file_list[index]], [collected_list[index]], [variable_list[index]], [label_list[index]], [Description_list[index]], [embeddings_Description[index]]]\n",
    "            collection.insert(entities)\n",
    "            print(f\"编号{id}插入完毕\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca458d7-e6e5-4a06-872e-e98ddac3b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')  \n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "def nltk_tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "    \n",
    "def remove_stopwords(text):\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return ' '.join(lemmatized_text)\n",
    "    \n",
    "def stem_tokens(token_list):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    return [stemmer.stem(token) for token in token_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b026cf-63c3-47cd-94ae-73967e250bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection\n",
    "from pymilvus import model\n",
    "\n",
    "sentence_transformer_ef = model.dense.SentenceTransformerEmbeddingFunction(\n",
    "    model_name='all-distilroberta-v1', # Specify the model name\n",
    "    device='cpu' # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n",
    ")\n",
    "\n",
    "Collection_Name = 'Milvus_Test_Sentrans'\n",
    "collection = Collection(Collection_Name)\n",
    "\n",
    "file_path = 'data_WHI.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "id_list = df.index.tolist()\n",
    "id_plus_one = [x + 1 for x in id_list]\n",
    "\n",
    "dataset_file_data = df[\"Dataset File\"].tolist()\n",
    "collected_data = df.Collected.tolist()\n",
    "variable_data = df.Variable.tolist()\n",
    "\n",
    "\n",
    "\n",
    "df['Label'] = df['Label'].fillna(' ')\n",
    "label_data = df['Label'].tolist()\n",
    "\n",
    "df['Description'] = df['Description'].fillna(' ')\n",
    "description_data = df['Description'].tolist()\n",
    "\n",
    "df['combine'] = df['Label'] + df['Description']\n",
    "df['combine'] = df['combine'].str.replace('[^a-zA-Z\\s]', '', regex=True)\n",
    "df['combine'] = df['combine'].str.lower()\n",
    "df['combine_no_stopwords'] = df['combine'].apply(remove_stopwords)\n",
    "df['lemmatize_tokens'] = df['combine_no_stopwords'].apply(lemmatize_text)\n",
    "combine_data = df['lemmatize_tokens'].tolist()\n",
    "# import ipdb;ipdb.set_trace()\n",
    "\n",
    "#embedding\n",
    "Info_embedding = sentence_transformer_ef.encode_documents(combine_data)\n",
    "\n",
    "\n",
    "# import ipdb;ipdb.set_trace()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    Insert_vector2(id_plus_one, dataset_file_data, collected_data, variable_data, label_data, description_data, Info_embedding, Collection = collection)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"插入数据时发生错误：{e}\")\n",
    "end_time = time.time()\n",
    "print(f\"插入数据用时为: {end_time - start_time}秒, 平均一条的插入时间为: {(end_time - start_time)/len(id_list)}\")\n",
    "print(f\"总共有: {len(id_list)}条数据\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
